\documentclass[10pt]{book}

\usepackage[russian]{babel}
\usepackage[utf8x]{inputenc}

\usepackage{amsmath,amssymb,amsthm}
\usepackage{hyperref}
\usepackage{fancyvrb}

\author{Глинских Георгий}
\title{Конспект лекций по строковым алгоритмам}

\theoremstyle{plain}
\newtheorem{stm}{Утверждение}[section]
\newtheorem{definition}{Определение}[section]

\newcommand{\textm}{\texttt{text}}
\newcommand{\patm}{\texttt{pat}}
\newcommand{\algm}[1]{\operatorname{#1}}
\newcommand{\linem}{\noindent\rule{\textwidth}{1pt}}

% Можно потом попробовать p или t
\newenvironment{figurem}[1][]
  {\begin{figure}[p]
  \caption{#1}
  \centering
  }
  {
  \end{figure}
  }

\DefineVerbatimEnvironment%
  {verbm}{Verbatim}
  {gobble=-2,numbers=left,numbersep=2mm,
  frame=single,framerule=1pt}

\begin{document}

\maketitle

\chapter{Поиск подстроки в строке}

\section{Вводные замечания}

Строки -- конечные последовательности символов над множеством, называемым
алфавитом. Здесь и далее я буду полагать, что алфавит -- множество чисел
$\{1, \ldots, \sigma\}$ и что строки индексируются с $1$\footnote{Так
удобнее для математиков, но хуже для программистов.}:
$$w = w(1)w(2)\ldots{}w(n) = w(1,n).$$

Введем понятия периода и грани строки.
\begin{definition}
  \label{period_dfn}
  Периодом строки назовем число $p$, такое, что
  $$\forall i : s(i) = s(i + p).$$ 
\end{definition}

\begin{stm}
  Следующие определения равнозначны:
  \begin{enumerate}
    \item $p$ -- период строки $w$.
    \item $s(p + 1, n)$ и суффикс, и префикс $w$.
    \item Существует слова $a$, $b$: $b$ префикс $a$ и $w = a^k b$.
  \end{enumerate}
\end{stm}
\begin{proof}

$1 \Rightarrow 2$. Посимвольно сравнивая, $s(1, n-p) = s(p + 1, n)$.

$2 \Rightarrow 3$. Положим $a = s(1, p)$. Тогда видно, что
$s(p + 1,2p) = s(1,p) = a$,
Аналогично $$\forall l : s((l-1)p + 1, lp) = a.$$
Остается взять $b = s(kp + 1, n)$, где $k$ -- наибольшее из чисел $l$.

$3 \Rightarrow 1$. Очевидно.

\end{proof}

\begin{definition}
  \label{border_dfn}
  Гранью строки назовем суффикс, одновременно являющийся префиксом. 
\end{definition}

\begin{stm}
  Число $0 < p \le n$ -- период $s$ согда $s(1, n-p)$ -- грань $s$.
\end{stm}
\begin{proof}
  Заметим, что $s(1, n-p) = s(p + 1, n)$ по условию, и тогда достаточно
  воспользоваться пунктом 2 предыдущего утверждения.
\end{proof}

\begin{quote}
  Важно мыслить объекты, которые обрабатываются алгоритмами, очень (очень!)
  большой размерности. Тогда будет появляться нужная интуиция.
\end{quote}

\section{Алгоритм Крошмора-Перрена}

При помощи периода можно улучшить обычный алгоритм поиска подстроки в строке.
Далее будем называть его Ord, его сложность
$T(\algm{Ord}) = O(|\textm||\patm|)$.

\begin{quote}
  Инвариант цикла в Ord -- вхождения с началом в $\textm(1, i)$ рассмотрены.
\end{quote}

\begin{figurem}[Обычный алгоритм поиска подстроки в строке (Ord)]
\begin{verbm}
  i = 1
  пока i + |pat| <= |text|:  # O(|text|)
    если text(i, i + |pat|) = pat(1, |pat|):  # O(|pat|)
      верни i
    i += 1
\end{verbm}
\end{figurem}

Пусть теперь известен $p$ -- минимальный период строки $\patm$. Тогда этот
алгоритм можно улучшить для нахождения серии вхождений
(вхождения, расположенные достаточно близко друг к другу). Заметим, что
вхождения находятся на расстоянии не меньше $p$ (иначе противоречие с 
минимальностью периода), следовательно, при
нахождениии вхождения, можно не смотреть на следущие $p$ возможных позиций.
Получим алгоритм Per.
\begin{quote}
  Инвариант цикла в Per -- для нахождения вхождения 
  остается рассмотреть $|\patm| - b + 1$ позиций. Иными словами,
  $\patm(1,b) = \textm(i, i+b)$.
\end{quote}

\begin{figurem}[Улучшенный алгоритм поиска подстроки в строке (Per)]
\begin{verbm}
  i = 1, b = 1
  пока i + |pat| <= |text|:  # O(|text| / p)
    c = max { c : pat(b, c) = text(i + b, i + c) }  # O(|pat|)
    если с < |pat|:
      i += 1, b = 1
    иначе:  # c = |pat|
      верни i
      i += p, b = |pat| - p + 1
\end{verbm}
\end{figurem}

Теперь рассмотрим наконец алгоритм Крошмора-Перрена. Его будем обозначать TW:
в англоязычной литературе его называют two-way алгоритмом. Он быстрый:
$T(\algm{TW}) = O(|\textm| + |\patm|), S(\algm{TW}) = O(1)$.

\begin{definition}
  Локальным периодом разложения $w = ab$ назовем число $q$ такое, что
  $$\forall i \in \{|a| - q, \ldots, |a|\}: \quad s(i) = s(i + q).$$
\end{definition}

Заметим, что число $p$ -- период $w$ будет локальным периодом для любого
разложения.

\begin{definition}
  Критическим назовем разложение $w = ab$, для которого минимальный локальный
  период совпадает с периодом строки $w$.
\end{definition}

\begin{stm}
  \label{crit_stm}
  У любой строки есть критическое разложение $w = ab$: $|a| = u < p$, где $p$
  -- период $w$. 
\end{stm}
\begin{proof}

Заметим сначала, что любой локальный период $q$ для критического разложения 
$(a,b)$ с условием $q < |b|$ делится на $p$. Из критичности, $q \ge p$. Тогда 
$(xy)^kx$ с $|xy| = p, |x| < p$ будет префиксом $b$. Но тогда $x$ будет 
суффиксом $a$ и префиксом $b$. Пришли к противоречию: удалось найти локальный
период $|x| < p \le q$.
  
\end{proof}

Рассмотрим алгоритм TW1. Идейно он работает так: зафиксируем критическое 
разложение $\patm = ab$ как в утверждении \ref{crit_stm} и
сначала пытаемся найти вхождение $b$ слева направо,
после чего проверим справа налево, будет ли это вхождение иметь вид
$ab = \patm$.

В начале каждой итерации главного цикла соблюдается инвариант
$\patm(1,b) = \textm(i,i+b)$ (аналогичено алгоритму Per).

\begin{figurem}[Алгоритм TW1]
\begin{verbm}
  # вход: text: str, pat: str, u: num, p: num 
  # ограничения: u < p, u критическая для pat, p - период pat
  i = 1, b = 1
  пока i + |par| <= |text|:
    c = max(u, b)
    d = max { d : pat(c, d) = text(i + c, i + d) }
    если d < |pat|:
      i += d - u
      b = 1
    иначе:  # d = |pat|
      c = u - 1
      d = min { d >= b : pat(d, c) = text(i + d, i + c) }
      если d < b:
        верни i
      i += p
      b = |pat| - p + 1
\end{verbm}
\end{figurem}

\begin{stm}
  \label{TW1_corr}
  Алгоритм TW1 находит все вхождения $\patm$ в $\textm$ со сложностью
  $$T(\algm{TW1}) = O(|\textm|), \quad S(\algm{TW1}) = O(1).$$
\end{stm}
\begin{proof}
  Допустим, $i$ пробегает позиции $\{i_k\}$, и в позиции $i_k < i' < i_{k+1}$
  было вхождение, которое мы не выведем. Одно из двух: мы при сканировании
  перескочили этот индекс или мы не распознали вхождение по этому индексу.

  Если мы перешли к $i_{k + 1}$ через строку 8, то мы не можем пропустить
  вхождение в силу критичности разложения и $|a| < p$. Формально,
  $i' - i_k < d - u$, тогда по утверждению \ref{crit_stm}, $i - i_k = Ap$.
  Следовательно, 
  $\textm(i + c) \ne \patm(c) = \patm(c - Ap) = \textm(i' - c - i')$.
  Противоречие.

  Если мы перешли к $i_{k+1}$ через строку 15, то мы не можем пропустить в
  силу того, что $p$ -- наименьший локальный период.

  Итак, алгоритм находит все вхождения.
  
  Мы используем лишь несколько переменных, откуда $S(\algm{TW1}) = O(1)$.
  Для оценки времени работы $T(\algm{TW1})$, заметим, что в 6 и 12 строке мы
  не касаемся символа дважды.
\end{proof}

Далее необходимо найти разложение и период. Идейно надо сделать пользоваться
алгоритмом $\algm{TW2}$.

\begin{figurem}[Алгоритм TW2]
\begin{verbm}
  # вход: крит. pat = ab, |a| < p. p' - период b.
  если a - суффикс b(1, p'):
    p = p'
    алгоритм TW1
  иначе:
    q = max {|a|, |b|} + 1
    алгоритм TW3.
\end{verbm}
\end{figurem}

\begin{figurem}[Алгоритм TW3]
\begin{verbm}
  # вход: text: str, pat: str, u: num, q: num 
  # ограничения: u < p, u критическая для pat
  i = 1
  пока i + |par| <= |text|:
    c = u
    d = max { d : pat(c, d) = text(i + c, i + d) }
    если d < |pat|:
      i += d - u
    иначе:  # d = |pat|
      c = u - 1
      d = min { d >= 0 : pat(d, c) = text(i + d, i + c) }
      если d = 0:
        верни i
      i += q
\end{verbm}
\end{figurem}

\begin{stm}
  Алгоритм TW3 находит все вхождения $\patm$ в $\textm$ со сложностью
  $$T(\algm{TW1}) = O(|\textm|), \quad S(\algm{TW1}) = O(1).$$
\end{stm}
\begin{proof}
  Ясно, что $p' < p$. Причем $p'$ -- период $\patm$ согда верно условие 
  в строке 2. Остальное доказательство повторяет доказательство утверждения
  \ref{TW1_corr}.
\end{proof}

\begin{stm}
  В алгоритме $\algm{TW2}$ $$q = max \{|a|, |b|\} + 1 < p.$$
\end{stm}
\begin{proof}
  Известно, что $|a| < p$. Достаточно показать, что $|b| < p$. Это так в силу
  того, что иначе если $p' | p$, то $p'$ -- локальный период. Иначе найдется
  период еще меньше, чем $p'$.
\end{proof}

Займемся необходимой предобработкой. Обозначим через $<$ лексикографический
порядок символов, а через $\lessdot$ -- обратный к лексикографическому.
Продолжим их на строки.

\begin{stm}[Волшебное разложение]
  Пусть $\patm = ab = cd$ где $b$ и $d$ -- лексикографически максимальные по
  $<$ и $\lessdot$ суффиксы. Если $|b| < |d|$, то $\patm = ab$ критичное.
  Иначе $\patm = cd$ критичное.
\end{stm}
\begin{proof}
  Если строка из одинаковых символов, то очевидно. Иначе не умаляя общности,
  пусть $|b| < |d|$.
  
  Заметим, что локальный период разложение $\patm = ab$ больше $|a|$. Ведь
  в противном случае можно прийти к противоречию с максимальностью $b$.

  Осталось показать, что $q$ -- минимальный период для разложения $\patm = ab$
  будет также периодом $\patm$. Используем такой факт: для любых строк $x, y$:
  $x < y$ и $x \lessdot y$, $x$ будет префиксом $y$. Используем его для строк
  $\patm(|c|+q,\cdot)$ и $\patm(|c|,\cdot)$.

  Очевидно, $\patm(|c|+q,\cdot) \lessdot \patm(|c|,\cdot)$ в силу выбора
  $c$. По порядку $<$ первые символы совпадают, а для остальных порядок, как
  между $\patm(|a|, \cdot) = b$ и $\patm(|a| + q, \cdot)$. Получили
  требуемое условие.
\end{proof}

Осталось найти максимальные суффиксы по заданным порядкам. Для этого используем
модификацию алгоритма Дюваля ($\algm{TW4}$). Идейно: читаем строку слева направа
и для каждого префикса находим максимальный суффикс и его период.

\begin{stm}[Дюваль]
  Пусть $\patm(i,k)$ -- максимальный суффикс строки $\patm(1,k)$.
  \begin{enumerate}
    \item Если $\patm((k+1) - p) = \patm(k+1)$, то $\patm(i,k+1)$ новый суффикс,
      его период $p$.
    \item Если $\patm((k+1) - p) > \patm(k+1)$, то $\patm(i,k+1)$ новый суффикс,
      его период $k - i$ (тривиальный).
    \item Если $\patm((k+1) - p) < \patm(k+1)$, то максимальный суффикс не может
      начинаться в позициях $[0, i + \{d: d|p, d > r \}]$.
  \end{enumerate}
\end{stm}
\begin{proof}
  Случай 1 очевиден. В остальных случаях от противного, и рассматриваем грани.
  Там можно вывести противоречие с максимальностью суффикса на предыдущем шаге.
\end{proof}

\begin{figurem}[Алгоритм TW4 (Дюваля)]
\begin{verbm}
  # вход: pat: str
  p = <j - i>
  i = 1, j = 2
  пока j <= |pat|:
    d = max { j + d <= |pat| : pat(i, i + d) = pat(j, j + d) }
    если j + d > |pat|: выйди из цикла
    если pat(i + d) > pat(j + d): j += d + 1
    иначе: i += (d / p + 1), j = i + 1
  верни pat(i, j - 1), p
\end{verbm}
\end{figurem}

Для оценки сложности заметим, что значение $2i + j$ за $k$ итераций 
увеличивается не меньше, чем на $k$.

\chapter{Поиск нескольких подстрок в строке}

\section{Алгоритм Ахо-Корасик}

Назовем множество строк $D$, которое будем называть словарем. Задача состоит в
нахождении всех вхождений всех слов из $D$ в строку $\textm$.

Задача решается в два этапа: составление вспомогательной структуры данных $P$
и обработка строки $\textm$. Есть наивный алгоритм $\algm{TR}$: в нем в
качестве вспомогательной структуры берется бор, а вершины - структуры:
\begin{verbm}
  root : V = < корень бора >
  
  v : V = {
    .repr = < строка на пути root - v >  # формально
    .term = < .repr является словом словаря D >
    .next = < (-): char -> V : .next(c).repr = .repr + c >
  }
\end{verbm}

\begin{figurem}[Алгоритм TR]
\begin{verbm}
  v = root
  для i = 1; i < |text| и v != nil; i += 1:
    если v.term: нашли слово v.repr в позиции i
    v = v.next(text(i))
\end{verbm}
\end{figurem}

Сложность алгоритма $\algm{TR}$ будет зависить от реализации $(-).next$. Если
использовать отображение или отсортированный массив пар, то время доступа
$O(\log \sigma)$. Если lookup-таблицу или массив, то $O(1)$. Для удобства будем
представлять вершины числами $\{1, 2, \ldots\}$ и что $.next$ реализован
lookup-таблицей.

Улучшим алгоритм $TR$: давайте добавим поле $v.link$, в котором
будем указываем на вершину для которой $u.repr$ будет наидлиннейшим суффиксом из
$v.repr$. $v.report$ будет указывать на вершину, которая при спуске по $v.link$
будет удовлетворять $.term = true$. Тогда получаем алгоритм Ахо-Корасик
$\algm{AHK}$

\begin{figurem}[Алгоритм AHK]
\begin{verbm}
  v = root
  для i = 1; i < |text| и v != nil; i += 1:
    пока v != root и v.next(text(i)) = nil: v = v.link
    v = v.next(text(i))
    если v = nil:
      v = root
    для u = v; u != root; u = u.report:
      если u.term:
        нашли слово u.repr в позиции i - |u.repr|
\end{verbm}
\end{figurem}

\begin{stm}
  Алгоритм $\algm{AHK}$ работает корректно и
  $$T(\algm{AHK}) = O(|\textm| \log \sigma + |D|)$$
\end{stm}
\begin{proof}
  Каждая итерация первого цикла увеличивает $j = i - |v.repr| \le i$. Тогда он
  отработает за $O(|\textm| \log \sigma)$. Второй цикл каждый раз вызывается
  для разных слов, так что он будет вызван $O(|D|)$ раз.

  Корректность следует из того, что $\algm{AHK}$ -- просто модицикация алгоритма
  $\algm{TR}$.
\end{proof}

Осталось научиться находить поле $v.link$. Для проверки максимального символа мы
можем отрезать по одному символу $v.repr$ и смотреть, в $u.repr$ какой вершины
мы перешли. Получим алгоритм $\algm{AHK1}$.

\begin{stm}
  $T(\algm{AHK1}) = O(\log \sigma \sum_{d \in D} |d|)$.
\end{stm}
\begin{proof}
  Это верно из того, что на пути $root-<v:v.repr = d>$ суммарное время работы для
  всех вершин будет $O(|d|)$:
  $$\sum n_i \le \sum |v_i.link.repr| - |v_d.link| + 1 = |v_d.link.repr| + |d|$$
\end{proof}

\begin{figurem}[Алгоритм AHK1]
\begin{verbm}
  qu(1) = root.link = root.report = root
  k = 2;
  для i = 1; i < k; i += 1:
    для v,c из { (v,c) qu(i).next(c) = v }:
      qu[k++] = v
      v.link = nil
      для p = qu[i]; p.report != nil и v.link = nil; p = p.link:
        v.link = p.link.next(c)
      если v.link = nil:
        v.link = root
      v.report = v.link
      пока !v.report.term:
        v.report = v.link.report
\end{verbm}
\end{figurem}

Если в словаре только одно слово, то получаем алгорит Кнута-Морриса-Пратта.
Тогда вместо дерева достаточно использовать массив значений. Вариант выше
занимает $S(\algm{AHK}) = O(|V|)$ памяти, а автоматный вариант $O(\sigma|V|)$
памяти.

\chapter{Строковые индексы}

\section{Суффиксный массив}

По строке $\textm$ необходимо построить структуру данных, с помощью которой
можно находить вхождения строки $\patm$ в строку $\textm$.

\begin{definition}
  Cуффиксный массив $SA$ содержит перестановку чисел $\{1,2,\ldots,|\textm|\}$,
  причем $\textm(SA(i), \cdot) < \textm(SA(j), \cdot)$ для $i < j$.
\end{definition}

\subsection{Алгоритм Карккайнена-Сандерса}

Допустим, что алфавит представлен числами $\{1,2,\ldots,|\textm|\}$.
Идейно можно бить строку на фрагменты длины $2, 4, 8, \ldots$ и сортировать
их слиянием: тогда сложность
$T(n) = O(n) + T({n \over 2}) = cn \sum_k {1 \over 2^k} = O(n)$.

Идейно: рекурсивно отсортируем все суфииксы: сначала на позициях не кратных
трем, затем оставшиеся и выполним слияние.

Пусть на каком-то этапе
отсортированными оказываются строки $t_1, t_2$ (порязрядной сортировкой).
И в строке $t_1$ все тройки меньше, чем в строке $t_2$. Пронумеруем тройки
числами $\{1, {2 \over 3}|\textm|\}$. Сформируем строку $t_1 \$ t_2$ с
бесконечно малым символом \$. Рассмотрим суффиксный массив $SA_{12}$ для этой
строки. Его занесем в $ISA_{12}(SA(x)) = x$.

Для сортировки оставшихся троек: если первые символы различны, то порядки
определяются непосредственно, иначе -- по построенным порядкам. Тогда,
получается, достаточно отсортировать пары $(\textm(3k), ISA_{12}(k))$.

Для слияния так же переиспользуем порядок, зафиксированный в $ISA_{12}$.

\section{Cуффиксное дерево}

\begin{definition}
  Массив $LCP$, содержит на позиции $i$ наибольший общего префикс строк
  $\textm(SA(i), \cdot)$ и $\textm(SA(i + 1), \cdot)$
\end{definition}

Если построить структуру,
вычисляющую $\min \{ LCP(x) : i < x < j \}$ за $O(1)$, то можно построить
наибольший общий префикс любой пары суффиксов за $O(1)$. $LCP$ можно построить
за $O(|\textm|)$ используя $SA, ISA$ по алгоритму Касаи и др.

\begin{definition}
  Суффиксный бор -- бор, в котором пути отмеченны всеми подстроками строки
  $\textm$. Терминальными вершинами считаются суффиксы.
\end{definition}

\begin{definition}
  Суффиксным деревом (сжатым суффиксным бором) назовем суффиксный бор без
  вершин с одним сыном. При удалении вершин соответствующие метки ребер
  склеиваются.
\end{definition}

Метками ребер оказываются подстроки. Будем хранить указатели на них. Тогда
вершины являются структурами:
\begin{verbm}
  root : V = < корень бора >
  
  v : V = {
    .repr = < строка на пути root - v >  # формально
    .term = < .repr является суффиксом text >
    .next = < по символу переходим в вершину,
              на ребре до которой данный символ первый>
    .par = < родитель >
    .beg, .end = < text(.beg, .end) = str(.par, v) >
  }
\end{verbm}

\begin{quote}
  Из определения полей $.beg, .end$, очевидно, у суффиксного дерева
  $|V| \le 2|\textm|$.
\end{quote}

Суффиксное дерево можно построить по $SA$ и $LCP$. Для этого надо вставлять
суффиксы в порядке $(\textm(SA(1), \cdot), \textm(SA(2), \cdot), \ldots)$.
При вставке очередного суффикса $\textm(SA(1), \cdot)$, при вставке поднимаемся
снизу этого пути, создавая вершину на нужном месте.

\subsection{Упрощенный алгорим Вейнера}

Идейно: надо добавлять по одному суффиксу, двигаясь справа налево по строке
$\textm$. На каждом шаге создаем новую вершину, и крепим ее к какой-то вершине
или разбиваем ребро для ее прикрепления.

\begin{definition}
  Префиксной ссылкой назовем ссылку на вершину по символу, для которой на пути
  из корня до вершины строка получается дописыванием данного символа к текущей
  строке.
\end{definition}

Заметим, что префиксных ссылок $O(|\textm|)$, ведь в каждую вершину ведет не
более одной ссылки. Для удобства еще оказывается удобным ввести фиктивный
корень: он будет родителем корня, и длина строки на $fake-root$ равна 1.

\begin{stm}
  У каждой вершины на пути для нового суффикса есть вершина на пути для
  старого суффикса, для которой префиксная ссылка ведет в нее.
\end{stm}
\begin{proof}
  Заметим, что путь для нового суффикса тот же, что и для старого, если
  удалить первый символ на пути.

  Возможны две ситуации: если нижняя вершина на новом пути терминальная, то
  мы нашли требуюмею вершину: достаточно прикрепить к ней.

  Иначе, в нижней вершине ветвление. Тогда пользуемся переносом одной ветви на
  другую.
\end{proof}

Теперь начнем вставлять суффикс.

\begin{stm}
  Если вершина существует или не существует в дереве, то есть вершина, которая
  по новому символу префиксной ссылкой переводит нас в новую вершину. Для вершин
  ниже таких ссылок не существует.
\end{stm}
\begin{proof}
  Из предыдущего утверждения, если вершина, к которой будем крепиться,
  существует, то условие выполняется.

  Если не существует, то если бы нарушалось второе условие, имело бы место
  противоречие: вершина не существует, а к ней прикреплена еще вершина.
\end{proof}

Если нужно разбивать ребро, то используя утверждение, спускаемся по одному
символу, и в момент, когда символы различаются, создаем вершину и крепим к ней
новый суффикс. Важно: при спуске вниз достаточно сверять только первый символ:
по утверждению.

Оценим время работы алгоритма. Если мы при подъеме просматриваем $k_i$ вершин,
то совокупная сложность будет $O(\sum k_i \log \sigma)$. Если длина старой
верви $m$, а новой $m'$, то $k - 4 \le m - m'$. Тогда сложность
$$\sum k_i < \sum (m - m' + 4) = 4|\textm| + m_|\textm| \le O(|\textm|).$$

\subsection{Алгоритм Укконена}

Пусть в каждой вершине храняться все прежние поля, и новые:
\begin{verbm}
  root : V = < корень бора >
  
  v : V = {
    .repr, .term, .next, .beg, .end

    .slink = < суффиксная ссылка >
  }
\end{verbm}

\begin{definition}
  Суффиксная ссылка указывает на вершину, у которой строка на пути
  получается добавлением одного символа к пути к заданной вершине.

  Для корная там можно хранить вспомогательные значения.
\end{definition}

\begin{stm}
  Суффиксная ссылка определена на любой некорневой вершине.
\end{stm}
\begin{proof}
  Доказательство аналогично доказательству первого утверждения у алгоритма
  Вейнера.
\end{proof}

Мы будем последовательно добавлять префиксы строки $\textm$.

При добавлении нового префикса терминальное состояние меняется. Поэтому будем
хранить только листья, корень, и вершины, у которых больше двух сыновей.
\begin{quotation}
  Инвариант: на каждом шаге известен максимальный неуникальный суффикс.
  Для его хранения будем хранить вершину, и длину ребра в нее от родителя.
\end{quotation}

\begin{stm}
  Суффиксы максимального неуникального суффикса тоже неуникальны.
\end{stm}

\begin{stm}
  Суффиксное дерево будет совпадать с бором, построенным на строке, а число
  листьев в боре будет равно длине максимального неуникального суффикса.
\end{stm}
\begin{proof}
  Суффиксы короче максимального неуникального не будут префиксами строки.
  Следовательно, в боре число листьев совпадает с длиной максимального
  неуникального суффикса.

  Для неуникального суффикса длины больше максимального неуникального суффикса
  будет иметь самое левое вхождение, тогда он будет префиксом части строки, и
  попадет в бор.
\end{proof}

Теперь при добавлении нового префикса в дерево, добавится $m - m'$ новых
листьев, где $m, m'$ -- старая и новая длина максимального суффикса.

Удобно полагать, что $.end = \infty$. Тогда нужно будет только вставлять новые
листья, а дописывание символов на ребрах происходит автоматически. Так же
необязательно хранить суффиксные ссылки.

Если не нужно обновлять максимальный суффикс, то достаточно добавить вершину,
разбив ребро ниже от запомненной вершины. Иначе для просмотра всех кандидатов на
максимальные суффиксы спускаемся по $.slink$ (в случае чего, спускаясь вниз по
ребрам), и добавляем вершину по одному новому символу, то тех пор, как только
переход будет по одному символу переход от строки.

Получили алгоритм $\operatorname{UOK}$.

\begin{figurem}[Алгоритм Укконена (UOK1)]
\begin{verbm}
  # разбиение ребра
  # вход: w, v, len, c
  u = new_node {.beg = v.beg, .end=v.beg+len}
  u.next(c) = new_node {.beg=i-1, .end=+inf}
  u.next(text(v.beg+len)) = v
  w.next(text(v.beg)) = u
  v.beg += len  # => v.len -= len
  верни u
\end{verbm}
\end{figurem}

\begin{figurem}[Алгоритм Укконена (UOK)]
\begin{verbm}
  # добавление символа с
  k++
  prev = root
  пока k > 0:
    для v = w.next(text(i-k)); v != null и k > v.len:
      w = v
      k -= w.len
      v = w.next(text(i-k))
    если v = null:
      w.next(text(i - k)) = new_node {.beg=i-k, .end=+inf}
      prev = prev.slink = w
    иначе если text(v.beg + k) != c:
      prev = prev.slink = <разбиение ребра>(w, v, k-1, c)
    иначе:
      prev.slink = w
      закончить
    если w != root: w = w.slink
    иначе k--
\end{verbm}
\end{figurem}

Оценим время работы алгоритма UOK. Пусть у нас есть указатели $p_1, p_2$, т.е.
длина максимального неуникального суффикса и его длина + длина пути до w + 1.
Каждая итерация цикла пока увелививает $p_1$ ровно на один, а $p_2$ на каждой
итерации для увеличивается хотя бы на один. $p_1 \le p_2$, следовательно, оба
цикла использует не более $2n$ операций. Тогда итоговое время работы
$O(n \log \sigma)$.

\subsection{Возможные оптимизации}

Можно использовать не указатели и new, а использовать статический массив
достаточного размера. Вместо указателей тогда можно просто использовать индексы.

В алгоритме Вейнера стоит завести стек.

Для реализации $.next, .link$ стоит завести глобальную таблицу, которая будет
по $V \times char$ определять значение.

\section{Индексы}

\section{FM-индекс}

\begin{definition}
  Преобразование Барроуза-Уилера по строке $\textm$ образует строку BWT, такую,
  что $$ BWT(i) = 
    \begin{cases}
      \textm(SA(i) - 1), &SA(i) = 0, \\
      \textm(n), &SA(i) \ne 0.
    \end{cases}$$
\end{definition}

Если в к строке добавить символ конца строки, то по построенному массиву можно
восстановить исходный текст. Далее будем работать со строками с таким символом.

\begin{definition}
  Рангом $rank(a, i)$ назовем число символов $a$ в строке BWT до
  позиции $i$ невключительно.
\end{definition}

Тогда BWT можно закодировать массивом $runs_a(i) = \{.beg, .rank\}$, где 
$.beg$ -- начало очередной серии, $.rank$ -- длина этой серии. Его можно
вычислить с помощью бинарного поиска.

Введем еще массив $C$ на символах: $C(a) = \sum_{b < a} |j : \textm(j) = b|$.

FM-индекс состоит из массивов $runs_a, C$. Будем предполагать, что
$\sigma << r$, где $r$ -- общее число серий. Размер FM-индекса будет $O(r)$.

\subsection{Алгоритм обратного поиска}

Идейно: считывая строку справа налево, находим самый длинный интервал, все
суффиксы из которого начинаются на прочитанную строку. Полученный алгоритм
назовем алгоритмом обратного поиска (RS).

\begin{figurem}[Алгоритм обратного поиска (RS)]
\begin{verbm}
  l = 1, r = n
  для i = n, n-1, ...:
    a = text(i)
    l = C(a) + rank(a, l)
    r = C(a) + rank(a, r)
\end{verbm}
\end{figurem}

\begin{stm}
  Алгоритм RS корректен и работает за $O(n \log r)$.
\end{stm}
\begin{proof}
  Мысленно можем продлить символы из BWT теми суффиксами, которые их продолжают.
  Тогда верно, что 
  $$l_i = C(a) + rank(a, l_{i+1}), \quad r_i - l_i = rank(a, r_{i+1}) - rank(a, l_{i+1}).$$
  Отсюда следует корректность. Сложность очевидна.
\end{proof}

Введем в $rans_a(i)$ дополнительное поле 
$$.sa = SA(runs_a(i).beg + runs_a(i).len - 1).$$
Так же введем массивы $first, firstrun$, где будут запомнены первое вхождение
первого элемента серии и перевод индексов первого вхождения в индекс предыдущей
серии.


\end{document}